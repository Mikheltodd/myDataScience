{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c67efa4",
   "metadata": {},
   "source": [
    "# Data Centering\n",
    "\n",
    "Data centering involves subtracting the mean of a data set from each data point so that the new mean is 0. Mathematically, this looks like:\n",
    "\n",
    "Xcenteredi=Xi−μ\n",
    "\n",
    "where X_i is a datapoint and the Greek letter μ is the mean of all the X values.\n",
    "\n",
    "For example, let’s take a look at a data set of ages for five individuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64a1a543",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 34.0\n",
      "Centered Ages: [-10.   6.  -6. -12.  22.]\n",
      "Sum of Centered Ages: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ages = [24, 40, 28, 22, 56]\n",
    "ages_mean = np.mean(ages)\n",
    "centered_ages = ages - ages_mean\n",
    "print(f'Mean: {ages_mean}\\nCentered Ages: {centered_ages}\\nSum of Centered Ages: {sum(centered_ages)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d7b23",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "This centered data is useful because it tells us how far above or below the mean each data point is, giving us additional insight that we can’t get just by looking at the initial data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8689c495",
   "metadata": {},
   "source": [
    "# Data Scaling\n",
    "\n",
    "A common task for data analysts and scientists is to find trends in data by comparing features of data points. However, this task is made difficult when the features are on drastically different scales.\n",
    "\n",
    "For instance, let’s consider a data set containing two features, age and income.\n",
    "\n",
    "In general, a person’s age usually ranges from 0 to about 100 years. A person’s income, on the other hand, usually ranges from 0 to large amounts measured in the thousands of dollars. Clearly, age and income are two features that have vastly different ranges.\n",
    "\n",
    "This presents issues when trying to use many machine learning algorithms, which treat all dimensions equally regardless of their scale. The difference in one year of age is interpreted as exactly equal to the difference in one dollar of income. That makes no sense! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49691537",
   "metadata": {},
   "source": [
    "# Min-Max Normalization\n",
    "\n",
    "Min-max normalization is one of the most simple and common ways to scale data.\n",
    "\n",
    "For every feature in a data set, the minimum value of that feature is transformed into 0, the maximum value is transformed into 1, and every other value is transformed into a decimal between 0 and 1.\n",
    "\n",
    "Xnorm = (X−Xmin)/(Xmax−Xmin)\n",
    "\n",
    "One downside of min-max normalization is that it does not handle outliers very well. For example, if you have 99 values between 0 and 20, and one value is 100, then the 99 values will all be transformed to a value between 0 and 0.2 while the outlier is transformed to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2548cf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.25, 0.5, 0.75, 1.0]\n",
      "[0.0, 0.5, 1.0]\n"
     ]
    }
   ],
   "source": [
    "def min_max_normalize(lst):\n",
    "  minimum = min(lst)\n",
    "  maximum = max(lst)\n",
    "  normalized = []\n",
    "\n",
    "  # code goes here\n",
    "  for el in lst:\n",
    "    normalized.append((el - minimum)/(maximum - minimum))\n",
    "\n",
    "  return normalized\n",
    "\n",
    "# Uncomment these function calls to test your function:\n",
    "print(min_max_normalize([0, 25, 50, 75, 100]))\n",
    "# should print [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "print(min_max_normalize([10, 12, 14]))\n",
    "# should print [0.0, 0.5, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a2f503",
   "metadata": {},
   "source": [
    "# Standardization\n",
    "\n",
    "Standardization (also known as Z-score normalization) is another common data scaling technique.\n",
    "\n",
    "Standardization involves subtracting the mean of each observation and then dividing by the standard deviation:\n",
    "\n",
    "z=(value−mean)/stdev\n",
    "\n",
    "Once standardization is complete, all the features will have a mean of zero, a standard deviation of one, and therefore, the same scale.\n",
    "\n",
    "Unlike normalization, standardization does not have a bounding range. This means that even if you have outliers in your data, your standardized data will not be affected. Therefore, if your dataset has outliers, standardization is the preferred scaling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84fa7498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.4184397163120568, -0.7092198581560284, 0.0, 0.7092198581560284, 1.4184397163120568]\n",
      "[-1.2254901960784315, 0.0, 1.2254901960784315]\n"
     ]
    }
   ],
   "source": [
    "def standardize(lst, mean, std_dev):\n",
    "  standardized = []\n",
    "\n",
    "  # code goes here\n",
    "  for el in lst:\n",
    "    standardized.append((el - mean)/std_dev)\n",
    "\n",
    "  return standardized\n",
    "\n",
    "# Uncomment these function calls to test your standardize function:\n",
    "print(standardize([1, 2, 3, 4, 5], 3.0, 1.41))\n",
    "# should print [-1.418, -0.709, 0.0, 0.709, 1.418]\n",
    "print(standardize([10, 15, 20], 15.0, 4.08))\n",
    "# should print [-1.225, 0.0, 1.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4cb249",
   "metadata": {},
   "source": [
    "## When to Normalize vs. Standardize?\n",
    "\n",
    "Min-max normalization and standardization both have a similar goal of transforming features in data to have the same scale so that each feature is equally important. So when should you use min-max normalization vs. standardization?\n",
    "\n",
    "There is not always a clear answer. Both normalization and standardization have their strengths as well as their drawbacks. For example, if you need your data to be on a 0-1 scale, then it makes sense to use min-max normalization. If you have outliers in your data, then it is best to use standardization (Z-score normalization) since it does not have a bounding range like min-max normalization does.\n",
    "\n",
    "Keep in mind that not every data set requires normalization or standardization. If your data features do not have vastly different ranges, then scaling your data might not be necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15187bb8",
   "metadata": {},
   "source": [
    "## Python Implementation\n",
    "\n",
    "As you saw, it is possible to implement min-max normalization and standardization by writing your own Python functions. However, in practice, most data analysts and scientists use popular libraries such as scikit-learn, which makes it very easy to scale your data.\n",
    "\n",
    "For example, to normalize your data you can import MinMaxScaler from the sklearn.preprocessing package and then make a simple function call:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e1b7aa",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    " \n",
    "# read in data \n",
    "data = pd.read_csv('data.csv')\n",
    " \n",
    "# normalize data \n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb01690",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    " \n",
    "# read in data \n",
    "data = pd.read_csv('data.csv')\n",
    " \n",
    "# standardize data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a8e239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
